{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das CRISP DM Modell teilt sich in folgende Phasen auf:\n",
    "<li> Business understanding\n",
    "<li> Data understanding\n",
    "<li> Data preparation\n",
    "<li> Modeling\n",
    "<li> Evaluating\n",
    "<li> Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Module importieren\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Business understanding </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir betrachten das Titanic-Problem. Basierend auf Passagierdaten wollen wir vorhersagen, ob jemand den Untergang der Titanic überlebt hätte. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Data understanding </h2>\n",
    "Der Datensatz enthält Merkmale wie Geschlecht, Alter, Passagierklasse, Ticketpreis, und Anzahl der Familienmitglieder an Bord. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "df = pd.read_csv('titanic.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Welche Spalten sind im Datensatz vorhanden? \n",
    "# Wie viele Zeilen sind im Datensatz vorhanden?\n",
    "# Wie viele fehlende Werte sind im Datensatz vorhanden?\n",
    "# Welche Datentypen sind im Datensatz vorhanden?\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <td>891.0</td>\n",
       "      <td>446.000000</td>\n",
       "      <td>257.353842</td>\n",
       "      <td>1.00</td>\n",
       "      <td>223.5000</td>\n",
       "      <td>446.0000</td>\n",
       "      <td>668.5</td>\n",
       "      <td>891.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>891.0</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>891.0</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>714.0</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>0.42</td>\n",
       "      <td>20.1250</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>80.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>891.0</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>891.0</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>891.0</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.9104</td>\n",
       "      <td>14.4542</td>\n",
       "      <td>31.0</td>\n",
       "      <td>512.3292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count        mean         std   min       25%       50%    75%  \\\n",
       "PassengerId  891.0  446.000000  257.353842  1.00  223.5000  446.0000  668.5   \n",
       "Survived     891.0    0.383838    0.486592  0.00    0.0000    0.0000    1.0   \n",
       "Pclass       891.0    2.308642    0.836071  1.00    2.0000    3.0000    3.0   \n",
       "Age          714.0   29.699118   14.526497  0.42   20.1250   28.0000   38.0   \n",
       "SibSp        891.0    0.523008    1.102743  0.00    0.0000    0.0000    1.0   \n",
       "Parch        891.0    0.381594    0.806057  0.00    0.0000    0.0000    0.0   \n",
       "Fare         891.0   32.204208   49.693429  0.00    7.9104   14.4542   31.0   \n",
       "\n",
       "                  max  \n",
       "PassengerId  891.0000  \n",
       "Survived       1.0000  \n",
       "Pclass         3.0000  \n",
       "Age           80.0000  \n",
       "SibSp          8.0000  \n",
       "Parch          6.0000  \n",
       "Fare         512.3292  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistische Kennzahlen anzeigen\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menge der fehlenden Werte in den Spalten\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Data preparation </h2>\n",
    "Die Daten werden zur Modellierung vorbereitet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgrund dessen, dass bei 891 Datenpunkten 687 mal die Datenpunkte zur Kabine fehlen, werden das Feature \"Cabin\" zur weiteren Analyse negiert <br>\n",
    "Dies Spalte wird demnach zunächst aus dem Datensatz entfernt. Ebenfalls werden Spalten Ticket, PassengerId und Name entfernt, da diese als nicht relevant angesehen werden <br>\n",
    "Anschließend wird der Datensatz in ein Ziel- und Featuredatensatz aufgeteilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der Datensatz wird aufgeteilt in die Zielvariable und die Features\n",
    "df_target = df['Survived']\n",
    "df = df.drop(columns = [\"Name\", \"Survived\", \"PassengerId\", \"Cabin\", \"Ticket\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir ersetzen die fehlenden Werte in der Spalte \"Age\" durch den Medianwert der Spalte\n",
    "median_value = df['Age'].median() \n",
    "df['Age'] = df['Age'].fillna(median_value)\n",
    "\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die kategorischen Variablen werden in numerische Variablen umgewandelt\n",
    "encoder = LabelEncoder()\n",
    "categories = ['Sex', 'Embarked']\n",
    "\n",
    "for i in categories:\n",
    "    df[i] = encoder.fit_transform(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.26512593 0.24494388 0.14233986 0.12044949 0.09520221 0.07958354\n",
      " 0.05235509]\n",
      "Number of Components: 7\n",
      "       Pclass       Sex       Age     SibSp     Parch      Fare  Embarked\n",
      "PC1 -0.533524 -0.343565  0.144889  0.188123  0.291205  0.614392 -0.280455\n",
      "PC2  0.343899 -0.221583 -0.499894  0.541222  0.514095 -0.047083  0.152671\n",
      "PC3 -0.200386  0.311234  0.408524  0.240118  0.192209  0.090660  0.770238\n",
      "PC4 -0.003976  0.817780 -0.179913  0.327167 -0.034936  0.248950 -0.358632\n",
      "PC5  0.286730  0.012434  0.685997  0.190487  0.371309 -0.326345 -0.407901\n",
      "PC6  0.029428  0.247588 -0.138630 -0.678285  0.670581  0.093072  0.014296\n",
      "PC7  0.688357 -0.076023  0.200107 -0.103142 -0.158717  0.659505  0.097803\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Choose the number of components (e.g., explain 95% variance)\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "print(f\"Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Number of Components: {pca.n_components_}\")\n",
    "\n",
    "\n",
    "# Get the PCA components\n",
    "pca_components = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=df.columns,  # Original feature names\n",
    "    index=[f'PC{i+1}' for i in range(pca.n_components_)]  # PC names\n",
    ")\n",
    "\n",
    "print(pca_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spalte Emvarked wird entfernt\n",
    "df = df.drop(columns=[\"SibSp\", \"Parch\", \"Embarked\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erkenntnisse der PCA:\n",
    "<li>Pclass: Hat starke Beiträge zu PC1 und PC2, was sie zu einer wichtigen Variable macht, die beibehalten werden sollte.\n",
    "<li>Sex: Trägt stark zu PC4 bei und hat auch Einfluss auf die Varianz, also sollte diese Variable ebenfalls beibehalten werden.\n",
    "<li>Fare: Wichtiger Einfluss auf PC1 und sollte beibehalten werden.\n",
    "<li>Age: Zeigt einen moderaten Einfluss auf mehrere PCs und könnte daher auch wichtig sein.\n",
    "<li>Parch: Diese Variable hat in mehreren PCs (besonders in den späteren) geringe Ladewerte und scheint daher einen geringen Einfluss auf die Hauptkomponenten zu haben.\n",
    "<li>SibSp: Auch diese Variable hat gemischte Ladewerte, aber insgesamt scheint sie weniger Einfluss auf die Varianz zu haben, die von den ersten PCs erklärt wird.\n",
    "<li>Embarked: Diese Variable zeigt in PC3 eine starke positive Ladung (0.770), aber in den anderen PCs schwächere Einflüsse und kann daher eher vernachlässigt werden <br>\n",
    "<br><b>Schlussfolgerung:</b> Die Merkmale Parch, Sibsp & Embarked werden ebenfalls aus dem Datensatz entfernt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Modeling & Evaluierung </h2>\n",
    "Es wird der K-nearest-neighbours Algorithmus angwendet, um die Daten zu modellieren. <br>\n",
    "Für jeden Durchlauf des Modells wird eine Evaluierung durchgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [27, 6728, 49122]\n",
    "features = df\n",
    "target = df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 27\n",
      "Cross-Validation Accuracy (Train): 0.8024\n",
      "Test Accuracy: 0.8209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86       167\n",
      "           1       0.81      0.68      0.74       101\n",
      "\n",
      "    accuracy                           0.82       268\n",
      "   macro avg       0.82      0.79      0.80       268\n",
      "weighted avg       0.82      0.82      0.82       268\n",
      "\n",
      "Random State: 6728\n",
      "Cross-Validation Accuracy (Train): 0.7847\n",
      "Test Accuracy: 0.8396\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       168\n",
      "           1       0.78      0.79      0.79       100\n",
      "\n",
      "    accuracy                           0.84       268\n",
      "   macro avg       0.83      0.83      0.83       268\n",
      "weighted avg       0.84      0.84      0.84       268\n",
      "\n",
      "Random State: 49122\n",
      "Cross-Validation Accuracy (Train): 0.7897\n",
      "Test Accuracy: 0.8022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84       170\n",
      "           1       0.71      0.78      0.74        98\n",
      "\n",
      "    accuracy                           0.80       268\n",
      "   macro avg       0.79      0.80      0.79       268\n",
      "weighted avg       0.81      0.80      0.80       268\n",
      "\n",
      "Mean Test Accuracy: 0.8209\n",
      "Std Test Accuracy: 0.0152\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "\n",
    "for RANDOM_STATE in states:\n",
    "    # Daten aufteilen\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=0.3, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Skalieren\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Modell erstellen\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    \n",
    "    # Cross-Validation durchführen\n",
    "    cv_scores = cross_val_score(knn, X_train, y_train, cv=10)\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    \n",
    "    # Modell trainieren und testen\n",
    "    knn.fit(X_train, y_train)\n",
    "    preds = knn.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, preds)\n",
    "    accs.append(test_accuracy)\n",
    "    \n",
    "    # Ergebnisse ausgeben\n",
    "    print(f\"Random State: {RANDOM_STATE}\")\n",
    "    print(f\"Cross-Validation Accuracy (Train): {mean_cv_score:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(classification_report(y_test, preds, zero_division=0.0))\n",
    "\n",
    "# Durchschnittliche Genauigkeit und Standardabweichung\n",
    "accs_mean = round(np.mean(accs), 4)\n",
    "accs_std = round(np.std(accs), 4)\n",
    "\n",
    "print(f\"Mean Test Accuracy: {accs_mean}\")\n",
    "print(f\"Std Test Accuracy: {accs_std}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Deployment </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Zunächst wurden die Daten aufbereitet, um ein Verständnis für sie zu entwickeln. Es lagen Daten im numerischen und kategorischen Format vor\n",
    "<li>Wir haben verschiedene Datensätze ausprobiert und getestet, um die höchste Genauigkeit zu erzielen, und sind zu dem Schluss gekommen, dass die Merkmale Pclass, Sex, Age und Fare die besten Ergebnisse liefern\n",
    "<li>Die anderen Merkmale wurden aus dem Modell entfernt\n",
    "<li>Das Modell wird für drei verschiedene Random States durchgeführt, um es auf Robustheit zu überprüfen\n",
    "<li>Dadurch, dass alle Random States ähnliche Ergebnisse liefern, kann das Modell als Robust und als nicht anfällig für overfitting angesehen werden\n",
    "<li>Die Ergebnisse der KNN-Modelle zur Vorhersage des Überlebens des Titanic Untergangs zeigen eine durchschnittliche Testgenauigkeit von 82,09% mit geringer Schwankung (±1,5%). Die beste Genauigkeit wurde bei Random State 6728 mit einer Accuracy von 84% erreicht.</li>\n",
    "<li>Die Cross-Validation-Ergebnisse unterstützen diese Beobachtungen, wobei das Modell für Random State 6728 eine durchschnittliche Genauigkeit von 78,47% über die Validierungsfolds erreichte. Dies zeigt, dass das Modell auch in anderen Datenaufteilungen robust ist\n",
    "<li>Das Modell zeigt eine starke Leistung bei der Erkennung von Nicht-Überlebenden (Klasse 0) mit einem Recall von von 82 - 90 % und einer Präzision von 83 - 87 %. Die Vorhersage von Überlebenden (Klasse 1) ist schwächer</li>\n",
    "<li>Insgesamt liefert das KNN-Modell robuste Vorhersagen, besonders für Nicht-Überlebende, mit Potenzial zur Optimierung bei der Erkennung von Überlebenden.</li>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
