{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d94c839-26aa-4f59-8a67-af1babee2b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 493.30588844178254\n",
      "Epoch 100, Loss: 471.90726523143894\n",
      "Epoch 200, Loss: 476.1174536427964\n",
      "Epoch 300, Loss: 498.32426817718544\n",
      "Epoch 400, Loss: 540.8366124945887\n",
      "Epoch 500, Loss: 583.0799794522964\n",
      "Epoch 600, Loss: 613.592397858941\n",
      "Epoch 700, Loss: 632.5931566016475\n",
      "Epoch 800, Loss: 643.6349845316676\n",
      "Epoch 900, Loss: 649.9307315622932\n",
      "Accuracy: 81.01%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Relativer Pfad zur Datei\n",
    "file_path = \"titanic.csv\"\n",
    "\n",
    "# Datei laden\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Unnötige Spalten entfernen\n",
    "data_cleaned = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1).copy()\n",
    "\n",
    "# Fehlende Werte behandeln (ohne Kopie-Warnung)\n",
    "data_cleaned.loc[:, 'Age'] = data_cleaned['Age'].fillna(data_cleaned['Age'].median())\n",
    "data_cleaned.loc[:, 'Embarked'] = data_cleaned['Embarked'].fillna(data_cleaned['Embarked'].mode()[0])\n",
    "\n",
    "# Kategorische Variablen umwandeln (One-Hot-Encoding)\n",
    "categorical_features = ['Sex', 'Embarked']\n",
    "numerical_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "X = data_cleaned.drop('Survived', axis=1)\n",
    "y = data_cleaned['Survived']\n",
    "\n",
    "# Daten vorverarbeiten\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Train-/Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Aktivierungsfunktionen\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Initialisierung des neuronalen Netzes\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    weights = {\n",
    "        \"W1\": np.random.randn(input_size, hidden_size) * 0.01,\n",
    "        \"b1\": np.zeros((1, hidden_size)),\n",
    "        \"W2\": np.random.randn(hidden_size, output_size) * 0.01,\n",
    "        \"b2\": np.zeros((1, output_size)),\n",
    "    }\n",
    "    return weights\n",
    "\n",
    "# Vorwärtspropagation\n",
    "def forward_propagation(X, weights):\n",
    "    Z1 = np.dot(X, weights[\"W1\"]) + weights[\"b1\"]\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, weights[\"W2\"]) + weights[\"b2\"]\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "# Verlustfunktion (Binary Cross Entropy)\n",
    "def compute_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    loss = -(1 / m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "# Rückwärtspropagation\n",
    "def backward_propagation(X, y, weights, cache):\n",
    "    m = X.shape[0]\n",
    "    A2 = cache[\"A2\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "\n",
    "    dZ2 = A2 - y.reshape(-1, 1)\n",
    "    dW2 = (1 / m) * np.dot(A1.T, dZ2)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "    dZ1 = np.dot(dZ2, weights[\"W2\"].T) * sigmoid_derivative(cache[\"Z1\"])\n",
    "    dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    gradients = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return gradients\n",
    "\n",
    "# Parameter-Update\n",
    "def update_weights(weights, gradients, learning_rate):\n",
    "    weights[\"W1\"] -= learning_rate * gradients[\"dW1\"]\n",
    "    weights[\"b1\"] -= learning_rate * gradients[\"db1\"]\n",
    "    weights[\"W2\"] -= learning_rate * gradients[\"dW2\"]\n",
    "    weights[\"b2\"] -= learning_rate * gradients[\"db2\"]\n",
    "    return weights\n",
    "\n",
    "# Training des Netzwerks\n",
    "def train_neural_network(X_train, y_train, input_size, hidden_size, output_size, epochs, learning_rate):\n",
    "    weights = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Vorwärtspropagation\n",
    "        y_pred, cache = forward_propagation(X_train, weights)\n",
    "\n",
    "        # Verlust berechnen\n",
    "        loss = compute_loss(y_train, y_pred)\n",
    "\n",
    "        # Rückwärtspropagation\n",
    "        gradients = backward_propagation(X_train, y_train, weights, cache)\n",
    "\n",
    "        # Gewichte aktualisieren\n",
    "        weights = update_weights(weights, gradients, learning_rate)\n",
    "\n",
    "        # Fortschritt ausgeben\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Testen des Netzwerks\n",
    "def predict(X, weights):\n",
    "    y_pred, _ = forward_propagation(X, weights)\n",
    "    return (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Hyperparameter und Training\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "epochs = 1000\n",
    "learning_rate = 0.25\n",
    "\n",
    "# Trainieren des Modells\n",
    "weights = train_neural_network(X_train, y_train.to_numpy(), input_size, hidden_size, output_size, epochs, learning_rate)\n",
    "\n",
    "# Vorhersage und Evaluierung\n",
    "y_pred = predict(X_test, weights)\n",
    "accuracy = np.mean(y_pred.flatten() == y_test.to_numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "#Zusammenfassung:\n",
    "# In diesem Projekt haben wir ein neuronales Netz von Grund auf implementiert, um die Überlebenswahrscheinlichkeit von Titanic-Passagieren vorherzusagen. \n",
    "# Nach der Vorverarbeitung der Daten, bei der unnötige Spalten entfernt, fehlende Werte behandelt und die Daten standardisiert wurden, \n",
    "# haben wir ein einfaches Netzwerk mit einer versteckten Schicht und Sigmoid-Aktivierung erstellt. \n",
    "# Die Vorwärts- und Rückwärtspropagation sowie die Aktualisierung der Gewichte wurden manuell programmiert. \n",
    "# Der Trainingsprozess basierte auf Gradientenabstieg, und das Modell wurde anhand der Genauigkeit auf Testdaten bewertet.\n",
    "\n",
    "# Verbesserungsmöglichkeiten:\n",
    "# Der Code kann durch stabilere Verlustberechnungen, bessere Initialisierung der Gewichte und Regularisierungsmethoden wie L2 oder Dropout weiter verbessert werden, \n",
    "# um Überanpassung zu vermeiden. Zusätzlich könnten modernere Optimierungsalgorithmen wie Adam und Visualisierungen des Verlustverlaufs hinzugefügt werden, \n",
    "# um den Trainingsprozess transparenter zu gestalten. Abschließend wäre es hilfreich, weitere Metriken wie Precision und Recall zu integrieren, \n",
    "# um die Modellleistung genauer zu bewerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25435ac7-c423-42b9-b44d-8be6e42f8a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Epoch 0, Loss: 493.3182564142499\n",
      "Epoch 100, Loss: 474.3337215002331\n",
      "Epoch 200, Loss: 478.5074699906682\n",
      "Epoch 300, Loss: 501.90913630407385\n",
      "Epoch 400, Loss: 546.7750293315195\n",
      "Epoch 500, Loss: 591.010039744103\n",
      "Epoch 600, Loss: 622.7424733658241\n",
      "Epoch 700, Loss: 642.3946529324004\n",
      "Epoch 800, Loss: 653.8160877763995\n",
      "Epoch 900, Loss: 660.4325295162313\n",
      "Accuracy for Fold 1: 78.21%\n",
      "\n",
      "Fold 2\n",
      "Epoch 0, Loss: 494.01240853553196\n",
      "Epoch 100, Loss: 475.2943125962951\n",
      "Epoch 200, Loss: 479.64029289598614\n",
      "Epoch 300, Loss: 504.58848544395244\n",
      "Epoch 400, Loss: 552.1219372479734\n",
      "Epoch 500, Loss: 598.2715969063767\n",
      "Epoch 600, Loss: 630.6530705848091\n",
      "Epoch 700, Loss: 650.1197099385706\n",
      "Epoch 800, Loss: 661.1294131037565\n",
      "Epoch 900, Loss: 667.3137906935644\n",
      "Accuracy for Fold 2: 80.34%\n",
      "\n",
      "Fold 3\n",
      "Epoch 0, Loss: 494.01241115044763\n",
      "Epoch 100, Loss: 475.35133772759224\n",
      "Epoch 200, Loss: 480.63533512556285\n",
      "Epoch 300, Loss: 509.6960338532325\n",
      "Epoch 400, Loss: 561.7895615700019\n",
      "Epoch 500, Loss: 610.3503698996842\n",
      "Epoch 600, Loss: 643.5515105836655\n",
      "Epoch 700, Loss: 662.9175969916919\n",
      "Epoch 800, Loss: 673.4754515343466\n",
      "Epoch 900, Loss: 679.2547249252082\n",
      "Accuracy for Fold 3: 78.65%\n",
      "\n",
      "Fold 4\n",
      "Epoch 0, Loss: 494.012672249953\n",
      "Epoch 100, Loss: 475.31673875171805\n",
      "Epoch 200, Loss: 479.95740400084884\n",
      "Epoch 300, Loss: 505.8746670021591\n",
      "Epoch 400, Loss: 555.6582805531912\n",
      "Epoch 500, Loss: 604.7357623350783\n",
      "Epoch 600, Loss: 639.1557659283927\n",
      "Epoch 700, Loss: 659.7530281478506\n",
      "Epoch 800, Loss: 671.3932826767597\n",
      "Epoch 900, Loss: 677.9500866289105\n",
      "Accuracy for Fold 4: 78.09%\n",
      "\n",
      "Fold 5\n",
      "Epoch 0, Loss: 494.0101667653433\n",
      "Epoch 100, Loss: 474.81969773004806\n",
      "Epoch 200, Loss: 479.1067158581296\n",
      "Epoch 300, Loss: 503.21953677358636\n",
      "Epoch 400, Loss: 548.9645299466481\n",
      "Epoch 500, Loss: 593.4985175087578\n",
      "Epoch 600, Loss: 624.7637251878798\n",
      "Epoch 700, Loss: 643.3840684707468\n",
      "Epoch 800, Loss: 653.8493980231209\n",
      "Epoch 900, Loss: 659.8020544357902\n",
      "Accuracy for Fold 5: 82.02%\n",
      "\n",
      "Cross-Validation Accuracy: 79.46%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Relativer Pfad zur Datei\n",
    "file_path = \"C:\\\\Users\\\\marti\\\\Downloads\\\\Neuronales Netz\\\\titanic.csv\"\n",
    "\n",
    "# Datei laden\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Unnötige Spalten entfernen\n",
    "data_cleaned = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1).copy()\n",
    "\n",
    "# Fehlende Werte behandeln\n",
    "data_cleaned.loc[:, 'Age'] = data_cleaned['Age'].fillna(data_cleaned['Age'].median())\n",
    "data_cleaned.loc[:, 'Embarked'] = data_cleaned['Embarked'].fillna(data_cleaned['Embarked'].mode()[0])\n",
    "\n",
    "# Kategorische Variablen umwandeln (One-Hot-Encoding)\n",
    "categorical_features = ['Sex', 'Embarked']\n",
    "numerical_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "X = data_cleaned.drop('Survived', axis=1)\n",
    "y = data_cleaned['Survived']\n",
    "\n",
    "# Daten vorverarbeiten\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Aktivierungsfunktionen\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Initialisierung des neuronalen Netzes\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    weights = {\n",
    "        \"W1\": np.random.randn(input_size, hidden_size) * 0.01,\n",
    "        \"b1\": np.zeros((1, hidden_size)),\n",
    "        \"W2\": np.random.randn(hidden_size, output_size) * 0.01,\n",
    "        \"b2\": np.zeros((1, output_size)),\n",
    "    }\n",
    "    return weights\n",
    "\n",
    "# Vorwärtspropagation\n",
    "def forward_propagation(X, weights):\n",
    "    Z1 = np.dot(X, weights[\"W1\"]) + weights[\"b1\"]\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, weights[\"W2\"]) + weights[\"b2\"]\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "# Verlustfunktion (Binary Cross Entropy)\n",
    "def compute_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8  # Kleine Konstante für Stabilität\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    m = y_true.shape[0]\n",
    "    loss = -(1 / m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "# Rückwärtspropagation\n",
    "def backward_propagation(X, y, weights, cache):\n",
    "    m = X.shape[0]\n",
    "    A2 = cache[\"A2\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "\n",
    "    dZ2 = A2 - y.reshape(-1, 1)\n",
    "    dW2 = (1 / m) * np.dot(A1.T, dZ2)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "    dZ1 = np.dot(dZ2, weights[\"W2\"].T) * sigmoid_derivative(cache[\"Z1\"])\n",
    "    dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    gradients = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return gradients\n",
    "\n",
    "# Parameter-Update\n",
    "def update_weights(weights, gradients, learning_rate):\n",
    "    weights[\"W1\"] -= learning_rate * gradients[\"dW1\"]\n",
    "    weights[\"b1\"] -= learning_rate * gradients[\"db1\"]\n",
    "    weights[\"W2\"] -= learning_rate * gradients[\"dW2\"]\n",
    "    weights[\"b2\"] -= learning_rate * gradients[\"db2\"]\n",
    "    return weights\n",
    "\n",
    "# Training des Netzwerks\n",
    "def train_neural_network(X_train, y_train, input_size, hidden_size, output_size, epochs, learning_rate):\n",
    "    weights = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Vorwärtspropagation\n",
    "        y_pred, cache = forward_propagation(X_train, weights)\n",
    "\n",
    "        # Verlust berechnen\n",
    "        loss = compute_loss(y_train, y_pred)\n",
    "\n",
    "        # Rückwärtspropagation\n",
    "        gradients = backward_propagation(X_train, y_train, weights, cache)\n",
    "\n",
    "        # Gewichte aktualisieren\n",
    "        weights = update_weights(weights, gradients, learning_rate)\n",
    "\n",
    "        # Fortschritt ausgeben\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Testen des Netzwerks\n",
    "def predict(X, weights):\n",
    "    y_pred, _ = forward_propagation(X, weights)\n",
    "    return (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Hyperparameter\n",
    "input_size = X_preprocessed.shape[1]\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "epochs = 1000\n",
    "learning_rate = 0.25\n",
    "\n",
    "# Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_accuracies = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_preprocessed, y)):\n",
    "    print(f\"\\nFold {fold + 1}\")\n",
    "    X_train, X_test = X_preprocessed[train_index], X_preprocessed[test_index]\n",
    "    y_train, y_test = y.to_numpy()[train_index], y.to_numpy()[test_index]\n",
    "\n",
    "    # Trainieren des Modells\n",
    "    weights = train_neural_network(X_train, y_train, input_size, hidden_size, output_size, epochs, learning_rate)\n",
    "\n",
    "    # Vorhersage und Evaluierung\n",
    "    y_pred = predict(X_test, weights)\n",
    "    accuracy = np.mean(y_pred.flatten() == y_test)\n",
    "    cv_accuracies.append(accuracy)\n",
    "    print(f\"Accuracy for Fold {fold + 1}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Durchschnittliche Genauigkeit\n",
    "mean_accuracy = np.mean(cv_accuracies)\n",
    "print(f\"\\nCross-Validation Accuracy: {mean_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0384168-c16a-43e1-b92d-f28f42ced6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
