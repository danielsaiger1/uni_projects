{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.signal import decimate\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_selection import select_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Files read:\n",
      "ce: (2205, 60)\n",
      "cp: (2205, 60)\n",
      "eps1: (2205, 6000)\n",
      "se: (2205, 60)\n",
      "vs1: (2205, 60)\n",
      "fs1: (2205, 600)\n",
      "fs2: (2205, 600)\n",
      "ps1: (2205, 6000)\n",
      "ps2: (2205, 6000)\n",
      "ps3: (2205, 6000)\n",
      "ps4: (2205, 6000)\n",
      "ps5: (2205, 6000)\n",
      "ps6: (2205, 6000)\n",
      "ts1: (2205, 60)\n",
      "ts2: (2205, 60)\n",
      "ts3: (2205, 60)\n",
      "ts4: (2205, 60)\n",
      "target: (2205, 5)\n"
     ]
    }
   ],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, input_path, file_names):\n",
    "        self.input_path = input_path\n",
    "        self.file_names = file_names\n",
    "        \n",
    "    def read_files(self):\n",
    "        self.data = {}\n",
    "        print(\"Reading files...\")\n",
    "        for file in self.file_names:\n",
    "            with open(self.input_path + file + '.txt', 'r') as f:\n",
    "                self.data[file] = pd.read_csv(f, header=None, sep='\\t')\n",
    "        return self.data\n",
    "    \n",
    "    def print_shape(self):\n",
    "        print(\"Files read:\")\n",
    "        for file in self.data:\n",
    "            print(f\"{file}: {self.data[file].shape}\")\n",
    "            \n",
    "    def create_target_df(self):\n",
    "        target_columns = ['Cooler_Condition', 'Valve_Condition', \n",
    "                        'Internal_Pump_Leakage', 'Hydraulic_Accumulator', \n",
    "                        'Stable_Flag']\n",
    "        self.data['target'].columns = target_columns\n",
    "        self.valve_condition = self.data['target']['Valve_Condition']\n",
    "        #del self.data['target']\n",
    "        return self.valve_condition\n",
    "\n",
    "def process_data():\n",
    "    input_path = \"input_data/\"\n",
    "    file_names = [\n",
    "        \"ce\", \"cp\", \"eps1\", \"se\", \"vs1\", \n",
    "        \"fs1\", \"fs2\", \n",
    "        \"ps1\", \"ps2\", \"ps3\", \"ps4\", \"ps5\", \"ps6\",\n",
    "        \"ts1\", \"ts2\", \"ts3\", \"ts4\", \"target\"\n",
    "    ]\n",
    "    \n",
    "    processor = DataProcessor(input_path, file_names)\n",
    "    data = processor.read_files()\n",
    "    processor.print_shape()\n",
    "    df_target = processor.create_target_df()\n",
    "    df_target = processor.valve_condition\n",
    "    return data, df_target\n",
    "\n",
    "data, df_target = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data['se']\n",
    "# df[\"id\"] = df.index\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsfel\n",
    "# Fenstergröße und Überlappung festlegen\n",
    "window_size = 10  # alle 10 Spalten sind ein Fenster\n",
    "overlap = 3  # 3 Spalten Überlappung\n",
    "\n",
    "window_df = df\n",
    "\n",
    "# Liste für die Fenster\n",
    "windows = []\n",
    "\n",
    "cfg = tsfel.get_features_by_domain(\"all\")\n",
    "# Schleife zur Fenstererstellung mit Überlappung\n",
    "for i, start_col in enumerate(range(0, window_df.shape[1] - window_size + 1, window_size - overlap)):\n",
    "    # Bestimme das Ende des Fensters\n",
    "    end_col = start_col + window_size\n",
    "    \n",
    "    # Wähle das Fenster aus den Daten\n",
    "    window = window_df.iloc[:, start_col:end_col]\n",
    "    \n",
    "    \n",
    "    # Füge die 'time'-Spalte hinzu\n",
    "    # window[\"id\"] = window.index\n",
    "    # Berechne die Standardabweichung pro Zeile\n",
    "    feature = tsfel.time_series_features_extractor(cfg, window, verbose=1)\n",
    "    \n",
    "    # Erstelle ein DataFrame mit dem Fensterindex\n",
    "    feature = pd.DataFrame(feature, columns=[f\"{i}\"])\n",
    "    # Fenster zur Liste hinzufügen\n",
    "    windows.append(feature)\n",
    "\n",
    "# Ergebnis\n",
    "print(f\"Anzahl der Fenster: {len(windows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          mean   median        std         var     max  min    iqr  skewness  \\\n",
      "2200  59.03310  68.4505  23.437563  549.319375  77.752  0.0  1.399 -2.055932   \n",
      "2201  59.06800  68.4075  23.440581  549.460855  77.592  0.0  1.223 -2.059495   \n",
      "2202  59.13235  68.3130  23.435632  549.228852  77.773  0.0  1.298 -2.069487   \n",
      "2203  58.97080  68.3500  23.403317  547.715231  77.568  0.0  1.220 -2.059104   \n",
      "2204  59.05390  68.3705  23.429505  548.941726  77.748  0.0  1.255 -2.061135   \n",
      "\n",
      "      kurtosis  row_index  \n",
      "2200  2.405264       2200  \n",
      "2201  2.414648       2201  \n",
      "2202  2.441802       2202  \n",
      "2203  2.413508       2203  \n",
      "2204  2.419783       2204  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "\n",
    "# Funktion zur Feature-Berechnung pro Zeile\n",
    "def extract_manual_features(row):\n",
    "    features = {}\n",
    "    features[\"mean\"] = np.mean(row)\n",
    "    features[\"median\"] = np.median(row)\n",
    "    features[\"std\"] = np.std(row)\n",
    "    features[\"var\"] = np.var(row)\n",
    "    features[\"max\"] = np.max(row)\n",
    "    features[\"min\"] = np.min(row)\n",
    "    features[\"iqr\"] = np.percentile(row, 75) - np.percentile(row, 25)\n",
    "    features[\"skewness\"] = skew(row)\n",
    "    features[\"kurtosis\"] = kurtosis(row)\n",
    "    return features\n",
    "# Feature-Extraktion für jede Zeile\n",
    "all_features = []\n",
    "for idx, row in df.iterrows():\n",
    "    row_features = extract_manual_features(row.values)\n",
    "    row_features[\"row_index\"] = idx  # Zeilenindex hinzufügen (optional)\n",
    "    all_features.append(row_features)\n",
    "\n",
    "# DataFrame mit extrahierten Features\n",
    "features_df = pd.DataFrame(all_features)\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(features_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2205,)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.shape\n",
    "df_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spaltennamen des ersten Fensters extrahieren\n",
    "reference_columns = windows[0].columns.tolist()\n",
    "\n",
    "# Angleichung der Spaltennamen aller Fenster an die des ersten Fensters\n",
    "for i in range(1, len(windows)):\n",
    "    windows[i].columns = reference_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_long = []\n",
    "for window in windows:\n",
    "    window_long = pd.melt(window, id_vars=['id'], var_name='time', value_name='value')\n",
    "    windows_long.append(window_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_long[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "for window in windows_long:\n",
    "    features = extract_features(\n",
    "        window,\n",
    "        column_id=\"id\",        # Zeitreihen-ID\n",
    "        column_sort=\"time\",    # Zeitstempel # Sensor-Typ\n",
    "        column_value=\"value\"   # Wert\n",
    "    )\n",
    "    feature_list.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feature_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_std = df.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = ['se', 'fs1', 'ps3']\n",
    "\n",
    "df_downsampled = {}\n",
    "\n",
    "for df in df_list:\n",
    "    filtered_signals = []  # Reset for each DataFrame\n",
    "    if data[df].shape[1] == 6000:\n",
    "        downsample_factor = 100\n",
    "        for i in range(data[df].shape[0]):\n",
    "            row = data[df].iloc[i].values  # Extract row as a 1D array\n",
    "            filtered_signal = decimate(row, downsample_factor, ftype='fir')  # Downsample\n",
    "            filtered_signals.append(filtered_signal)  # Store the result\n",
    "        # Create a new DataFrame with the filtered signals and add the 'id' column\n",
    "        df_downsampled[df] = pd.DataFrame(filtered_signals)\n",
    "        df_downsampled[df][\"id\"] = df_downsampled[df].index\n",
    "\n",
    "    elif data[df].shape[1] == 600:\n",
    "        downsample_factor = 10\n",
    "        for i in range(data[df].shape[0]):\n",
    "            row = data[df].iloc[i].values  # Extract row as a 1D array\n",
    "            filtered_signal = decimate(row, downsample_factor, ftype='fir')  # Downsample\n",
    "            filtered_signals.append(filtered_signal)  # Store the result\n",
    "        # Create a new DataFrame with the filtered signals and add the 'id' column\n",
    "        df_downsampled[df] = pd.DataFrame(filtered_signals)\n",
    "        df_downsampled[df][\"id\"] = df_downsampled[df].index\n",
    "\n",
    "    else:\n",
    "        df_downsampled[df] = data[df]\n",
    "        df_downsampled[df][\"id\"] = df_downsampled[df].index\n",
    "\n",
    "# Combine all DataFrames\n",
    "df_combined = pd.concat([df_downsampled[df] for df in df_list], ignore_index=True)\n",
    "\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), dpi=100)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, s in enumerate(df_list):\n",
    "    ax = axes[idx]\n",
    "    df = df_downsampled[s].drop(columns=['id'])  # Adjusted to use `s` and drop the column correctly\n",
    "    \n",
    "    for i in range(df.shape[0] - 1):\n",
    "        ax.plot(df.iloc[i], color='blue', linewidth=0.5, label=f'Series {i}' if i == 0 else \"\")  # Add label only once\n",
    "    \n",
    "    ax.set_title(s)\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Test\")\n",
    "\n",
    "    ax.legend(loc='best', fontsize='small')  # Legend specific to each subplot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fenstergröße und Überlappung festlegen\n",
    "window_size = 10  # alle 10 Spalten sind ein Fenster\n",
    "overlap = 3  # 3 Spalten Überlappung\n",
    "\n",
    "# Entferne 'id' und setze die Zielspalte 'id'\n",
    "window_df = df_combined.drop(columns=['id'])\n",
    "id = df_combined[\"id\"]\n",
    "\n",
    "# Liste für die Fenster\n",
    "windows = []\n",
    "\n",
    "# Schleife zur Fenstererstellung mit Überlappung\n",
    "for start_col in range(0, window_df.shape[1] - window_size + 1, window_size - overlap):\n",
    "    # Bestimme das Ende des Fensters\n",
    "    end_col = start_col + window_size\n",
    "    \n",
    "    # Wähle das Fenster aus den Daten\n",
    "    window = window_df.iloc[:, start_col:end_col]\n",
    "    \n",
    "    # Füge die 'time'-Spalte hinzu\n",
    "    window[\"id\"] = id\n",
    "    \n",
    "    # Fenster zur Liste hinzufügen\n",
    "    windows.append(window)\n",
    "\n",
    "# Ergebnis\n",
    "print(f\"Anzahl der Fenster: {len(windows)}\")\n",
    "\n",
    "# # Optional: Ausgabe eines Fensters zur Kontrolle\n",
    "# # print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spaltennamen des ersten Fensters extrahieren\n",
    "reference_columns = windows[0].columns.tolist()\n",
    "\n",
    "# Angleichung der Spaltennamen aller Fenster an die des ersten Fensters\n",
    "for i in range(1, len(windows)):\n",
    "    windows[i].columns = reference_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_combined = pd.concat([i for i in windows], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_long = pd.melt(windows_combined, id_vars=['id'], var_name='time', value_name='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor-Daten zusammenfügen\n",
    "df_downsampled[\"se\"][\"sensor\"] = \"sensor_1\"\n",
    "\n",
    "df_downsampled[\"fs1\"][\"sensor\"] = \"sensor_2\"\n",
    "\n",
    "df_downsampled[\"ps3\"][\"sensor\"] = \"sensor_3\"\n",
    "\n",
    "# Daten ins lange Format bringen\n",
    "sensor_1_long = df_downsampled[\"se\"].melt(id_vars=[\"id\", \"sensor\"], var_name=\"time\", value_name=\"value\")\n",
    "sensor_2_long = df_downsampled[\"fs1\"].melt(id_vars=[\"id\", \"sensor\"], var_name=\"time\", value_name=\"value\")\n",
    "sensor_3_long = df_downsampled[\"ps3\"].melt(id_vars=[\"id\", \"sensor\"], var_name=\"time\", value_name=\"value\")\n",
    "\n",
    "# Alle Sensor-Daten kombinieren\n",
    "all_sensors_long = pd.concat([sensor_1_long, sensor_2_long, sensor_3_long])\n",
    "\n",
    "all_sensors_long\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merkmalsextraktion mit TSFresh\n",
    "features = extract_features(\n",
    "    all_sensors_long,\n",
    "    column_id=\"id\",        # Zeitreihen-ID\n",
    "    column_sort=\"time\",    # Zeitstempel\n",
    "    column_kind=\"sensor\",  # Sensor-Typ\n",
    "    column_value=\"value\"   # Wert\n",
    ")\n",
    "\n",
    "print(features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = extract_features(window_long, \n",
    "                                        column_id=\"id\", \n",
    "                                        column_sort=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the features\n",
    "extracted_features = extracted_features.dropna(axis=1)\n",
    "extracted_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "extracted_features = extracted_features.dropna(how = \"all\", axis= \"columns\")\n",
    "# Feature-Selektion basierend auf Zielwerten\n",
    "extracted_features = select_features(extracted_features, y=y_encoded)\n",
    "selector = VarianceThreshold()\n",
    "extracted_features = selector.fit_transform(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape of feature and target\n",
    "if extracted_features.shape[0] == y_encoded.shape[0]:\n",
    "    print(\"Data is ready for Modelling!\")\n",
    "    print(f\"Shape features: {extracted_features.shape}\")\n",
    "    print(f\"Shape target: {y_encoded.shape}\")\n",
    "else:\n",
    "    print(\"Shape of the Inputs and target don't match. Please check preprocesing steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [27, 6728, 49122]\n",
    "features = features_df\n",
    "target = df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\anaconda3\\envs\\py_adv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          73       0.94      0.93      0.94        72\n",
      "          80       0.22      0.82      0.35        72\n",
      "          90       0.22      0.03      0.05        72\n",
      "         100       0.76      0.32      0.45       225\n",
      "\n",
      "    accuracy                           0.45       441\n",
      "   macro avg       0.54      0.52      0.44       441\n",
      "weighted avg       0.61      0.45      0.44       441\n",
      "\n",
      "Random State: 6728\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          73       0.58      0.97      0.73        72\n",
      "          80       0.06      0.19      0.10        72\n",
      "          90       0.71      0.07      0.13        72\n",
      "         100       0.70      0.28      0.40       225\n",
      "\n",
      "    accuracy                           0.35       441\n",
      "   macro avg       0.51      0.38      0.34       441\n",
      "weighted avg       0.58      0.35      0.36       441\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\anaconda3\\envs\\py_adv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Daniel\\anaconda3\\envs\\py_adv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 49122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          73       0.73      0.94      0.82        72\n",
      "          80       0.15      0.51      0.23        72\n",
      "          90       0.30      0.04      0.07        72\n",
      "         100       0.69      0.29      0.41       225\n",
      "\n",
      "    accuracy                           0.39       441\n",
      "   macro avg       0.47      0.45      0.39       441\n",
      "weighted avg       0.55      0.39      0.40       441\n",
      "\n",
      "Mean Accuracy: 0.3976\n",
      "Std Accuracy: 0.0426\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for RANDOM_STATE in states:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size = 0.2, random_state = RANDOM_STATE, stratify = target\n",
    "    )\n",
    "    \n",
    "    model = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    accs.append(accuracy_score(y_test, preds))\n",
    "    print(f\"Random State: {RANDOM_STATE}\")\n",
    "    print(classification_report(y_test, preds, zero_division=0.0))\n",
    "\n",
    "accs_mean = round(np.mean(accs), 4)\n",
    "accs_std = round(np.std(accs), 4)\n",
    "\n",
    "print(f\"Mean Accuracy: {accs_mean}\")\n",
    "print(f\"Std Accuracy: {accs_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "for RANDOM_STATE in states:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size = 0.2, random_state = RANDOM_STATE, stratify = target\n",
    "    )\n",
    "    \n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    accs.append(accuracy_score(y_test, preds))\n",
    "    print(f\"Random State: {RANDOM_STATE}\")\n",
    "    print(classification_report(y_test, preds, zero_division=0.0))\n",
    "\n",
    "accs_mean = round(np.mean(accs), 4)\n",
    "accs_std = round(np.std(accs), 4)\n",
    "\n",
    "print(f\"Mean Accuracy: {accs_mean}\")\n",
    "print(f\"Std Accuracy: {accs_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "for RANDOM_STATE in states:\n",
    " \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=0.2, random_state=RANDOM_STATE, stratify=target\n",
    "    )\n",
    "    \n",
    "    # Standardise features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    preds = knn.predict(X_test)  \n",
    "    accs.append(accuracy_score(y_test, preds))\n",
    "    \n",
    "    # Ergebnisse ausgeben\n",
    "    print(f\"Random State: {RANDOM_STATE}\")\n",
    "    print(classification_report(y_test, preds, zero_division=0.0))\n",
    "\n",
    "\n",
    "accs_mean = round(np.mean(accs), 4)\n",
    "accs_std = round(np.std(accs), 4)\n",
    "\n",
    "print(f\"Mean Accuracy: {accs_mean}\")\n",
    "print(f\"Std Accuracy: {accs_std}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
