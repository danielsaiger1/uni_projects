{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Files read:\n",
      "ce: (2205, 60)\n",
      "cp: (2205, 60)\n",
      "eps1: (2205, 6000)\n",
      "se: (2205, 60)\n",
      "vs1: (2205, 60)\n",
      "fs1: (2205, 600)\n",
      "fs2: (2205, 600)\n",
      "ps1: (2205, 6000)\n",
      "ps2: (2205, 6000)\n",
      "ps3: (2205, 6000)\n",
      "ps4: (2205, 6000)\n",
      "ps5: (2205, 6000)\n",
      "ps6: (2205, 6000)\n",
      "ts1: (2205, 60)\n",
      "ts2: (2205, 60)\n",
      "ts3: (2205, 60)\n",
      "ts4: (2205, 60)\n",
      "target: (2205, 5)\n"
     ]
    }
   ],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, input_path, file_names):\n",
    "        self.input_path = input_path\n",
    "        self.file_names = file_names\n",
    "        \n",
    "    def read_files(self):\n",
    "        self.data = {}\n",
    "        print(\"Reading files...\")\n",
    "        for file in self.file_names:\n",
    "            with open(self.input_path + file + '.txt', 'r') as f:\n",
    "                self.data[file] = pd.read_csv(f, header=None, sep='\\t')\n",
    "        return self.data\n",
    "    \n",
    "    def print_shape(self):\n",
    "        print(\"Files read:\")\n",
    "        for file in self.data:\n",
    "            print(f\"{file}: {self.data[file].shape}\")\n",
    "            \n",
    "    def create_target_df(self):\n",
    "        target_columns = ['Cooler_Condition', 'Valve_Condition', \n",
    "                        'Internal_Pump_Leakage', 'Hydraulic_Accumulator', \n",
    "                        'Stable_Flag']\n",
    "        self.data['target'].columns = target_columns\n",
    "        self.valve_condition = self.data['target']['Valve_Condition']\n",
    "        #del self.data['target']\n",
    "        return self.valve_condition\n",
    "\n",
    "def process_data():\n",
    "    input_path = \"input_data/\"\n",
    "    file_names = [\n",
    "        \"ce\", \"cp\", \"eps1\", \"se\", \"vs1\", \n",
    "        \"fs1\", \"fs2\", \n",
    "        \"ps1\", \"ps2\", \"ps3\", \"ps4\", \"ps5\", \"ps6\",\n",
    "        \"ts1\", \"ts2\", \"ts3\", \"ts4\", \"target\"\n",
    "    ]\n",
    "    \n",
    "    processor = DataProcessor(input_path, file_names)\n",
    "    data = processor.read_files()\n",
    "    processor.print_shape()\n",
    "    df_target = processor.create_target_df()\n",
    "    df_target = processor.valve_condition\n",
    "    return data, df_target\n",
    "\n",
    "data, df_target = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.concat([data['ce'], data['cp'], data['eps1'], data['se'], data['vs1'], \n",
    "                      data['fs1'], data['fs2'], data['ps1'], data['ps2'], data['ps3'], data['ps4'], \n",
    "                      data['ps5'], data['ps6'], data['ts1'], data['ts2'], data['ts3'], data['ts4']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoche 1/50, Verlust: 4.556509667209217\n",
      "Epoche 2/50, Verlust: 2.498163526611669\n",
      "Epoche 3/50, Verlust: 1.171979677996465\n",
      "Epoche 4/50, Verlust: 0.6447216091411454\n",
      "Epoche 5/50, Verlust: 0.5712702706888584\n",
      "Epoche 6/50, Verlust: 0.4669345053178923\n",
      "Epoche 7/50, Verlust: 0.5181969981640577\n",
      "Epoche 8/50, Verlust: 0.33220890475370524\n",
      "Epoche 9/50, Verlust: 0.35471872220348033\n",
      "Epoche 10/50, Verlust: 0.23822542925232223\n",
      "Epoche 11/50, Verlust: 0.24063675311793173\n",
      "Epoche 12/50, Verlust: 0.2475860056916385\n",
      "Epoche 13/50, Verlust: 0.2221258759299027\n",
      "Epoche 14/50, Verlust: 0.18540272720149784\n",
      "Epoche 15/50, Verlust: 0.13954871158681012\n",
      "Epoche 16/50, Verlust: 0.14804131289877528\n",
      "Epoche 17/50, Verlust: 0.28147868536939313\n",
      "Epoche 18/50, Verlust: 0.21276527429154157\n",
      "Epoche 19/50, Verlust: 0.16137418765785178\n",
      "Epoche 20/50, Verlust: 0.137578582066843\n",
      "Epoche 21/50, Verlust: 0.14777985878286667\n",
      "Epoche 22/50, Verlust: 0.19733251671173743\n",
      "Epoche 23/50, Verlust: 0.3783714731024312\n",
      "Epoche 24/50, Verlust: 0.16809159128128418\n",
      "Epoche 25/50, Verlust: 0.17764057678037457\n",
      "Epoche 26/50, Verlust: 0.2579002448889826\n",
      "Epoche 27/50, Verlust: 0.28134920708336203\n",
      "Epoche 28/50, Verlust: 0.2932545182805565\n",
      "Epoche 29/50, Verlust: 0.23095567700719194\n",
      "Epoche 30/50, Verlust: 0.2644174265109801\n",
      "Epoche 31/50, Verlust: 0.19233957273952132\n",
      "Epoche 32/50, Verlust: 0.39936624706855844\n",
      "Epoche 33/50, Verlust: 0.27759027254900764\n",
      "Epoche 34/50, Verlust: 0.13821308279875666\n",
      "Epoche 35/50, Verlust: 0.08412330656678282\n",
      "Epoche 36/50, Verlust: 0.08525906599554998\n",
      "Epoche 37/50, Verlust: 0.0735573235308818\n",
      "Epoche 38/50, Verlust: 0.07105908291122986\n",
      "Epoche 39/50, Verlust: 0.0999190146228232\n",
      "Epoche 40/50, Verlust: 0.15349651431357156\n",
      "Epoche 41/50, Verlust: 0.2323982806576948\n",
      "Epoche 42/50, Verlust: 0.3714852480749999\n",
      "Epoche 43/50, Verlust: 0.43527932272159625\n",
      "Epoche 44/50, Verlust: 0.17710780716203903\n",
      "Epoche 45/50, Verlust: 0.1352552664133582\n",
      "Epoche 46/50, Verlust: 0.07121907094083976\n",
      "Epoche 47/50, Verlust: 0.08829626956997215\n",
      "Epoche 48/50, Verlust: 0.08680258844313878\n",
      "Epoche 49/50, Verlust: 0.10381731761507089\n",
      "Epoche 50/50, Verlust: 0.0784973116990711\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        88\n",
      "           1       0.84      1.00      0.91        67\n",
      "           2       0.87      0.85      0.86        80\n",
      "           3       1.00      0.95      0.98       206\n",
      "\n",
      "    accuracy                           0.95       441\n",
      "   macro avg       0.93      0.95      0.94       441\n",
      "weighted avg       0.95      0.95      0.95       441\n",
      "\n",
      "Genauigkeit: 94.78%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Standardisiere die Eingabedaten\n",
    "scaler = StandardScaler()\n",
    "input_data_scaled = scaler.fit_transform(input_df)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df_target)\n",
    "\n",
    "# Splitte die Daten in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Konvertiere in Tensoren für PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Erstelle DataLoader für das Training\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# TimeMixer Modell Implementierung\n",
    "class TimeMixer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=128, num_layers=3, dropout=0.2):\n",
    "        super(TimeMixer, self).__init__()\n",
    "        \n",
    "        # Zeitmischungsschichten\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Aktivierungsfunktion\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Modell, Verlustfunktion und Optimierer\n",
    "model = TimeMixer(input_size=X_train.shape[1], output_size=4)  # Output size = Anzahl Klassen (z. B. 2 für binär)\n",
    "criterion = nn.CrossEntropyLoss()  # Verlustfunktion für Klassifikation\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Trainingsschleife\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Verlust berechnen\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward Pass und Optimierung\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoche {epoch+1}/{num_epochs}, Verlust: {running_loss / len(train_loader)}')\n",
    "\n",
    "# Evaluierung mit Classification Report\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Vorhersagen in Klassen umwandeln\n",
    "        preds = torch.argmax(outputs, axis=1)  # Maximale Wahrscheinlichkeit → Klasse\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds))\n",
    "\n",
    "# Optional: Genauigkeit berechnen\n",
    "accuracy = accuracy_score(all_targets, all_preds)\n",
    "print(f\"Genauigkeit: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
