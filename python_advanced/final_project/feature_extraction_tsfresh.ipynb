{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Manual Feature Extraction with Tsfresh </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Imports and load data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.signal import decimate\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_selection import select_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Files read:\n",
      "ce: (2205, 60)\n",
      "cp: (2205, 60)\n",
      "eps1: (2205, 6000)\n",
      "se: (2205, 60)\n",
      "vs1: (2205, 60)\n",
      "fs1: (2205, 600)\n",
      "fs2: (2205, 600)\n",
      "ps1: (2205, 6000)\n",
      "ps2: (2205, 6000)\n",
      "ps3: (2205, 6000)\n",
      "ps4: (2205, 6000)\n",
      "ps5: (2205, 6000)\n",
      "ps6: (2205, 6000)\n",
      "ts1: (2205, 60)\n",
      "ts2: (2205, 60)\n",
      "ts3: (2205, 60)\n",
      "ts4: (2205, 60)\n",
      "target: (2205, 5)\n"
     ]
    }
   ],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, input_path: str, file_names: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the DataProcessor with the given input path and file names.\n",
    "\n",
    "        Args:\n",
    "            input_path (str): The directory path where the files are stored.\n",
    "            file_names (List[str]): List of file names to be read.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.input_path = input_path\n",
    "        self.file_names = file_names\n",
    "        \n",
    "    def read_files(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Reads the files specified in the file names and stores them in a dictionary.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, pd.DataFrame]: A dictionary where keys are file names and values are DataFrames.\n",
    "        \"\"\"\n",
    "        self.data = {}\n",
    "        print(\"Reading files...\")\n",
    "        for file in self.file_names:\n",
    "            with open(self.input_path + file + '.txt', 'r') as f:\n",
    "                self.data[file] = pd.read_csv(f, header=None, sep='\\t')\n",
    "        return self.data\n",
    "    \n",
    "    def print_shape(self) -> None:\n",
    "        \"\"\"\n",
    "        Prints the shape of each loaded DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print(\"Files read:\")\n",
    "        for file in self.data:\n",
    "            print(f\"{file}: {self.data[file].shape}\")\n",
    "            \n",
    "    def create_target_df(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Renames the columns in the data['target'] and creates the wanted target DataFrame by extracting the column 'Valve_Condition'.\n",
    "\n",
    "        Returns:\n",
    "            pd.Series: A pandas Series containing the 'Valve_Condition' column from the target DataFrame.\n",
    "        \"\"\"\n",
    "        target_columns = ['Cooler_Condition', 'Valve_Condition', \n",
    "                          'Internal_Pump_Leakage', 'Hydraulic_Accumulator', \n",
    "                          'Stable_Flag']\n",
    "        self.data['target'].columns = target_columns\n",
    "        self.valve_condition = self.data['target']['Valve_Condition']\n",
    "        return self.valve_condition\n",
    "\n",
    "def process_data() -> (Dict[str, pd.DataFrame], pd.Series): # type: ignore\n",
    "    \"\"\"\n",
    "    Processes the data by reading the files and extracting the target DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, pd.DataFrame], pd.Series]: A tuple containing the data dictionary and the valve condition Series.\n",
    "    \"\"\"\n",
    "    input_path = \"input_data/\"\n",
    "    file_names = [\n",
    "        \"ce\", \"cp\", \"eps1\", \"se\", \"vs1\", \n",
    "        \"fs1\", \"fs2\", \n",
    "        \"ps1\", \"ps2\", \"ps3\", \"ps4\", \"ps5\", \"ps6\",\n",
    "        \"ts1\", \"ts2\", \"ts3\", \"ts4\", \"target\"\n",
    "    ]\n",
    "    \n",
    "    processor = DataProcessor(input_path, file_names)\n",
    "    data = processor.read_files()\n",
    "    processor.print_shape()\n",
    "    df_target = processor.create_target_df()\n",
    "    return data, df_target\n",
    "\n",
    "data, df_target = process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Signal Preprocessing </h2>\n",
    "\n",
    "<h3> Input data </h3>\n",
    "\n",
    "Steps:\n",
    "\n",
    "<ul>\n",
    "    <li>If the signal frequency is > 1 Hz, the signal gets downsampled to 1 Hz </li>\n",
    "    <li>Downsampled signals are stored in a new dictionary</li>\n",
    "    <li>An ID column gets added to the downsampled signals</li>\n",
    "    <li>The downsampled signals are concatenated in one dataframe</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_data(df_list: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downsamples each DataFrame in the provided 'df_list' by applying a FIR filter \n",
    "    using the decimate function and returns a combined DataFrame with all downsampled signals.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : Dict[str, pd.DataFrame]\n",
    "        Dictionary containing all the DataFrames loaded in the previous step.\n",
    "    \n",
    "    df_list : list\n",
    "        A list of keys (strings) that indicate which DataFrames from the 'data'\n",
    "        dictionary should be downsampled.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A combined DataFrame with downsampled signals from each DataFrame in the 'df_list'.\n",
    "        Each signal is downsampled based on its original shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_downsampled: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    for df in df_list:\n",
    "        filtered_signals = []\n",
    "        if data[df].shape[1] == 6000:\n",
    "            downsample_factor = 100\n",
    "            for i in range(data[df].shape[0]):\n",
    "                row = data[df].iloc[i].values  # extract row as a 1D array\n",
    "                filtered_signal = decimate(row, downsample_factor, ftype='fir')  # downsample\n",
    "                filtered_signals.append(filtered_signal)  \n",
    "            # create dataframe with downsampled signals and add 'id' column\n",
    "            df_downsampled[df] = pd.DataFrame(filtered_signals)\n",
    "            df_downsampled[df][\"id\"] = df_downsampled[df].index\n",
    "\n",
    "        elif data[df].shape[1] == 600:\n",
    "            downsample_factor = 10\n",
    "            for i in range(data[df].shape[0]):\n",
    "                row = data[df].iloc[i].values  # extract row as a 1D array\n",
    "                filtered_signal = decimate(row, downsample_factor, ftype='fir')  # downsample\n",
    "                filtered_signals.append(filtered_signal)  \n",
    "            # create dataframe with downsampled signals and add 'id' column\n",
    "            df_downsampled[df] = pd.DataFrame(filtered_signals)\n",
    "            df_downsampled[df][\"id\"] = df_downsampled[df].index\n",
    "\n",
    "        else:\n",
    "            df_downsampled[df] = data[df]\n",
    "            df_downsampled[df][\"id\"] = df_downsampled[df].index\n",
    "        \n",
    "    # Debugging: print shape of each downsampled DataFrame\n",
    "    for i in df_downsampled.keys():\n",
    "        print(f\"shape of {i}: {df_downsampled[i].shape}\")\n",
    "    \n",
    "    # Combine the downsampled signals into one dataframe\n",
    "    df_combined = pd.concat([df_downsampled[df] for df in df_list], ignore_index=True)\n",
    "\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of ps3: (2205, 61)\n",
      "shape of eps1: (2205, 61)\n"
     ]
    }
   ],
   "source": [
    "# Create the combined, downsampled dataframe\n",
    "df_list = ['ps3', 'eps1']\n",
    "df_combined = downsample_data(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Transform Input Data into long format </h3>\n",
    "\n",
    "Since Tsfresh needs the input data in the long format, we transform our input DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_long = pd.melt(df_combined, id_vars=['id'], var_name='time', value_name='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Target Data</h3>\n",
    "\n",
    "Encoding the target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Extract Features </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.95it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# using only the minimal important features\n",
    "settings = MinimalFCParameters()\n",
    "\n",
    "# extract the features\n",
    "extracted_features = extract_features(df_combined_long, \n",
    "                                      column_id=\"id\", \n",
    "                                      column_sort=\"time\", \n",
    "                                      default_fc_parameters=settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_features(feature_input: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the input feature DataFrame by performing the following steps:\n",
    "    1. Deletes columns with only NaN values.\n",
    "    2. Replaces infinite values (both positive and negative) with NaN.\n",
    "    3. Drops columns with only NaN values after replacement.\n",
    "    4. Uses the select_features function offered by tsfresh to identify relevant features based on a given target variable.\n",
    "    5. Applies variance thresholding to remove features with low variance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_input : pd.DataFrame\n",
    "        A DataFrame containing the input feature data.\n",
    "    \n",
    "    y_encoded : pd.Series\n",
    "        A Series representing the target or label data, used for feature selection (if needed).\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        The cleaned and transformed feature data, after dropping NaN and infinite values,\n",
    "        selecting relevant features, and applying variance thresholding.\n",
    "    \"\"\"\n",
    "    # Step 1: Delete columns with only NaN values\n",
    "    features_cleaned = feature_input.dropna(axis=1)\n",
    "    \n",
    "    # Step 2: Replace infinite values with NaN\n",
    "    features_cleaned = features_cleaned.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Step 3: Drop columns with only NaN values after replacing infinities\n",
    "    features_cleaned = features_cleaned.dropna(how=\"all\", axis=\"columns\")\n",
    "    \n",
    "    # Step 4: Use select_features() function to identify relevant features based on the target y_encoded.\n",
    "    features_cleaned = select_features(features_cleaned, y=y_encoded)\n",
    "    \n",
    "    # Step 5: Apply variance thresholding to remove low-variance features\n",
    "    selector = VarianceThreshold()\n",
    "    features_cleaned = selector.fit_transform(features_cleaned)\n",
    "    \n",
    "    return features_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = clean_features(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def check_data_readiness(feature_input: np.ndarray | pd.DataFrame, target_input: np.ndarray | pd.Series) -> None:\n",
    "    \"\"\"\n",
    "    Checks if the number of samples (rows) in the feature and target arrays/dataframes match.\n",
    "    If they match, prints the shape of both feature and target. Otherwise, prints an error message.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature : np.ndarray | pd.DataFrame\n",
    "        The feature data, either as a NumPy array or a Pandas DataFrame.\n",
    "        \n",
    "    target : np.ndarray | pd.Series\n",
    "        The target data, either as a NumPy array or a Pandas Series.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        This function does not return anything, it simply prints messages based on the shape check.\n",
    "    \"\"\"\n",
    "    if feature_input.shape[0] == target_input.shape[0]:\n",
    "        print(\"Data is ready for Modelling!\")\n",
    "        print(f\"Shape features: {feature_input.shape}\")\n",
    "        print(f\"Shape target: {target_input.shape}\")\n",
    "    else:\n",
    "        print(\"Shape of the Inputs and target don't match. Please check pre-processing steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready for Modelling!\n",
      "Shape features: (2205, 6)\n",
      "Shape target: (2205,)\n"
     ]
    }
   ],
   "source": [
    "check_data_readiness(features, y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Making predictions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [27, 6728, 49122]\n",
    "features = features\n",
    "target = y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.1. AdaBoost Classifier </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99        72\n",
      "           1       0.99      0.99      0.99        72\n",
      "           2       0.99      1.00      0.99        72\n",
      "           3       1.00      1.00      1.00       225\n",
      "\n",
      "    accuracy                           0.99       441\n",
      "   macro avg       0.99      0.99      0.99       441\n",
      "weighted avg       0.99      0.99      0.99       441\n",
      "\n",
      "Random State: 6728\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        72\n",
      "           1       1.00      1.00      1.00        72\n",
      "           2       0.97      0.97      0.97        72\n",
      "           3       0.99      0.99      0.99       225\n",
      "\n",
      "    accuracy                           0.99       441\n",
      "   macro avg       0.99      0.99      0.99       441\n",
      "weighted avg       0.99      0.99      0.99       441\n",
      "\n",
      "Random State: 49122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        72\n",
      "           1       1.00      0.99      0.99        72\n",
      "           2       0.99      1.00      0.99        72\n",
      "           3       1.00      1.00      1.00       225\n",
      "\n",
      "    accuracy                           1.00       441\n",
      "   macro avg       0.99      1.00      0.99       441\n",
      "weighted avg       1.00      1.00      1.00       441\n",
      "\n",
      "Mean Accuracy: 0.9932\n",
      "Std Accuracy: 0.0019\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for RANDOM_STATE in states:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size = 0.2, random_state = RANDOM_STATE, stratify = target\n",
    "    )\n",
    "    \n",
    "    model = AdaBoostClassifier(\n",
    "    algorithm='SAMME',\n",
    "    estimator=DecisionTreeClassifier(max_depth=3),\n",
    "    n_estimators=50\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    accs.append(accuracy_score(y_test, preds))\n",
    "    \n",
    "    # print classification report\n",
    "    print(f\"Random State: {RANDOM_STATE}\")\n",
    "    print(classification_report(y_test, preds, zero_division=0.0))\n",
    "\n",
    "accs_mean = round(np.mean(accs), 4)\n",
    "accs_std = round(np.std(accs), 4)\n",
    "\n",
    "print(f\"Mean Accuracy: {accs_mean}\")\n",
    "print(f\"Std Accuracy: {accs_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.2. K-Nearest Neighbours</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        72\n",
      "           1       1.00      0.97      0.99        72\n",
      "           2       0.96      1.00      0.98        72\n",
      "           3       1.00      0.99      1.00       225\n",
      "\n",
      "    accuracy                           0.99       441\n",
      "   macro avg       0.99      0.99      0.99       441\n",
      "weighted avg       0.99      0.99      0.99       441\n",
      "\n",
      "Random State: 6728\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        72\n",
      "           1       1.00      1.00      1.00        72\n",
      "           2       0.97      0.99      0.98        72\n",
      "           3       1.00      0.99      0.99       225\n",
      "\n",
      "    accuracy                           0.99       441\n",
      "   macro avg       0.99      0.99      0.99       441\n",
      "weighted avg       0.99      0.99      0.99       441\n",
      "\n",
      "Random State: 49122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        72\n",
      "           1       1.00      0.96      0.98        72\n",
      "           2       0.94      1.00      0.97        72\n",
      "           3       1.00      0.99      0.99       225\n",
      "\n",
      "    accuracy                           0.99       441\n",
      "   macro avg       0.98      0.99      0.98       441\n",
      "weighted avg       0.99      0.99      0.99       441\n",
      "\n",
      "Mean Accuracy: 0.9902\n",
      "Std Accuracy: 0.0028\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for RANDOM_STATE in states:\n",
    " \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=0.2, random_state=RANDOM_STATE, stratify=target\n",
    "    )\n",
    "    \n",
    "    # Standardise features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    preds = knn.predict(X_test) \n",
    "    accs.append(accuracy_score(y_test, preds))\n",
    "    \n",
    "    # print classification report\n",
    "    print(f\"Random State: {RANDOM_STATE}\")\n",
    "    print(classification_report(y_test, preds, zero_division=0.0))\n",
    "\n",
    "\n",
    "accs_mean = round(np.mean(accs), 4)\n",
    "accs_std = round(np.std(accs), 4)\n",
    "\n",
    "print(f\"Mean Accuracy: {accs_mean}\")\n",
    "print(f\"Std Accuracy: {accs_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.3. XGBoost Classifier</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for random state 27:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        72\n",
      "           1       1.00      0.99      0.99        72\n",
      "           2       0.99      1.00      0.99        72\n",
      "           3       1.00      1.00      1.00       225\n",
      "\n",
      "    accuracy                           1.00       441\n",
      "   macro avg       0.99      1.00      0.99       441\n",
      "weighted avg       1.00      1.00      1.00       441\n",
      "\n",
      "Classification Report for random state 6728:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        72\n",
      "           1       1.00      1.00      1.00        72\n",
      "           2       0.97      0.99      0.98        72\n",
      "           3       1.00      0.99      0.99       225\n",
      "\n",
      "    accuracy                           0.99       441\n",
      "   macro avg       0.99      0.99      0.99       441\n",
      "weighted avg       0.99      0.99      0.99       441\n",
      "\n",
      "Classification Report for random state 49122:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        72\n",
      "           1       1.00      0.99      0.99        72\n",
      "           2       0.97      0.97      0.97        72\n",
      "           3       0.99      0.99      0.99       225\n",
      "\n",
      "    accuracy                           0.99       441\n",
      "   macro avg       0.99      0.99      0.99       441\n",
      "weighted avg       0.99      0.99      0.99       441\n",
      "\n",
      "Mean Accuracy: 0.9924\n",
      "Std Accuracy: 0.0028\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for RANDOM_STATE in states:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=0.2, random_state=RANDOM_STATE, stratify=target\n",
    "    )\n",
    "\n",
    "\n",
    "    xgb_clf = XGBClassifier(n_estimators = 50,\n",
    "                            learning_rate = 0.05,\n",
    "                            eval_metric = \"logloss\",\n",
    "                            n_jobs = -1)\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "    preds = xgb_clf.predict(X_test)\n",
    "\n",
    "    accs.append(accuracy_score(y_test, preds))\n",
    "    \n",
    "    # print classification report\n",
    "    print(f\"Classification Report for random state {RANDOM_STATE}:\")\n",
    "    print(classification_report(y_test, preds, zero_division=0.0))\n",
    "\n",
    "accs_mean = round(np.mean(accs), 4)\n",
    "accs_std = round(np.std(accs), 4)\n",
    "\n",
    "print(f\"Mean Accuracy: {accs_mean}\")\n",
    "print(f\"Std Accuracy: {accs_std}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
